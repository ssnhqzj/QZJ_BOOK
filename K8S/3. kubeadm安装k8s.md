### 1. 准备两台主机

```bash
$ vim /etc/hosts
10.151.30.57 master
10.151.30.62 node01
```

### 2. 禁用防火墙
注：可查看Linux目录下“Ubuntu禁用防火墙”这篇文章

### 3. 禁用swap虚拟内存
注：可查看k8s目录下“Ubuntu禁用swap虚拟内存”这篇文章

### 4. 安装docker
注：可查看docker目录下“docker安装”这篇文章

### 5. 镜像
如果你的节点上面有科学上网的工具，可以忽略这一步，我们需要提前将所需的gcr.io上面的镜像下载到节点上面，
当然前提条件是你已经成功安装了`docker。master节点，执行下面的命令：
```bash
docker pull cnych/kube-apiserver-amd64:v1.10.0
docker pull cnych/kube-scheduler-amd64:v1.10.0
docker pull cnych/kube-controller-manager-amd64:v1.10.0
docker pull cnych/kube-proxy-amd64:v1.10.0
docker pull cnych/k8s-dns-kube-dns-amd64:1.14.8
docker pull cnych/k8s-dns-dnsmasq-nanny-amd64:1.14.8
docker pull cnych/k8s-dns-sidecar-amd64:1.14.8
docker pull cnych/etcd-amd64:3.1.12
docker pull cnych/flannel:v0.10.0-amd64
docker pull cnych/pause-amd64:3.1

docker tag cnych/kube-apiserver-amd64:v1.10.0 k8s.gcr.io/kube-apiserver-amd64:v1.10.0
docker tag cnych/kube-scheduler-amd64:v1.10.0 k8s.gcr.io/kube-scheduler-amd64:v1.10.0
docker tag cnych/kube-controller-manager-amd64:v1.10.0 k8s.gcr.io/kube-controller-manager-amd64:v1.10.0
docker tag cnych/kube-proxy-amd64:v1.10.0 k8s.gcr.io/kube-proxy-amd64:v1.10.0
docker tag cnych/k8s-dns-kube-dns-amd64:1.14.8 k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.8
docker tag cnych/k8s-dns-dnsmasq-nanny-amd64:1.14.8 k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.8
docker tag cnych/k8s-dns-sidecar-amd64:1.14.8 k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.8
docker tag cnych/etcd-amd64:3.1.12 k8s.gcr.io/etcd-amd64:3.1.12
docker tag cnych/flannel:v0.10.0-amd64 quay.io/coreos/flannel:v0.10.0-amd64
docker tag cnych/pause-amd64:3.1 k8s.gcr.io/pause-amd64:3.1
```
可以将上面的命令保存为一个 shell 脚本，然后直接执行即可。这些镜像是在 master 节点上需要使用到的镜像，一定要提前下载下来。

其他Node，执行下面的命令：
```bash
docker pull cnych/kube-proxy-amd64:v1.10.0
docker pull cnych/flannel:v0.10.0-amd64
docker pull cnych/pause-amd64:3.1
docker pull cnych/kubernetes-dashboard-amd64:v1.8.3
docker pull cnych/heapster-influxdb-amd64:v1.3.3
docker pull cnych/heapster-grafana-amd64:v4.4.3
docker pull cnych/heapster-amd64:v1.4.2
docker pull cnych/k8s-dns-kube-dns-amd64:1.14.8
docker pull cnych/k8s-dns-dnsmasq-nanny-amd64:1.14.8
docker pull cnych/k8s-dns-sidecar-amd64:1.14.8

docker tag cnych/flannel:v0.10.0-amd64 quay.io/coreos/flannel:v0.10.0-amd64
docker tag cnych/pause-amd64:3.1 k8s.gcr.io/pause-amd64:3.1
docker tag cnych/kube-proxy-amd64:v1.10.0 k8s.gcr.io/kube-proxy-amd64:v1.10.0

docker tag cnych/k8s-dns-kube-dns-amd64:1.14.8 k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.8
docker tag cnych/k8s-dns-dnsmasq-nanny-amd64:1.14.8 k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.8
docker tag cnych/k8s-dns-sidecar-amd64:1.14.8 k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.8

docker tag cnych/kubernetes-dashboard-amd64:v1.8.3 k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3
docker tag cnych/heapster-influxdb-amd64:v1.3.3 k8s.gcr.io/heapster-influxdb-amd64:v1.3.3
docker tag cnych/heapster-grafana-amd64:v4.4.3 k8s.gcr.io/heapster-grafana-amd64:v4.4.3
docker tag cnych/heapster-amd64:v1.4.2 k8s.gcr.io/heapster-amd64:v1.4.2
```
上面的这些镜像是在 Node 节点中需要用到的镜像，在 join 节点之前也需要先下载到节点上面。

### 6. 安装 kubeadm、kubectl、kubelet
设置国内下载源：
```bash
apt-get update && apt-get install -y apt-transport-https

curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 

cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF  

apt-get update
```

安装：
```bash
apt-get install -y kubelet=1.10.0-00 kubeadm=1.10.0-00 kubectl=1.10.0-00
```

### 7. 配置 kubelet
安装完成后，我们还需要对kubelet进行配置，因为用yum源的方式安装的kubelet生成的配置文件将参数--cgroup-driver改成了systemd，
而 docker 的cgroup-driver是cgroupfs，这二者必须一致才行，我们可以通过docker info命令查看：
```bash
$ docker info |grep Cgroup
Cgroup Driver: cgroupfs
```

修改文件 kubelet 的配置文件/etc/systemd/system/kubelet.service.d/10-kubeadm.conf，将其中的KUBELET_CGROUP_ARGS参数更改成
cgroupfs：
```bash
Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs"
```

另外还有一个问题是关于交换分区的，之前我们在手动搭建高可用的 kubernetes 集群一文中已经提到过，Kubernetes 从1.8开始要求关闭
系统的 Swap ，如果不关闭，默认配置的 kubelet 将无法启动，我们可以通过 kubelet 的启动参数--fail-swap-on=false更改这个限制，
所以我们需要在上面的配置文件中增加一项配置(在ExecStart之前)：
```bash
Environment="KUBELET_EXTRA_ARGS=--fail-swap-on=false"
```

当然最好的还是将 swap 给关掉，这样能提高 kubelet 的性能。修改完成后，重新加载我们的配置文件即可：
```bash
$ systemctl daemon-reload
```

### 8. 集群安装初始化
到这里我们的准备工作就完成了，接下来我们就可以在master节点上用kubeadm命令来初始化我们的集群了：
```bash
$ kubeadm init --kubernetes-version=v1.10.0 --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=10.151.30.57
# 10.151.30.57为master的IP
```

命令非常简单，就是kubeadm init，后面的参数是需要安装的集群版本，因为我们这里选择flannel作为 Pod 的网络插件，所以需要指定
–pod-network-cidr=10.244.0.0/16，然后是 apiserver 的通信地址，这里就是我们 master 节点的 IP 地址。执行上面的命令，
如果出现running with swap on is not supported. Please disable swap之类的错误，则我们还需要增加一个参数
–ignore-preflight-errors=Swap来忽略 swap 的错误提示信息：
 ```bash
$ kubeadm init \
   --kubernetes-version=v1.10.0 \
   --pod-network-cidr=10.244.0.0/16 \
   --apiserver-advertise-address=10.151.30.57 \
   --ignore-preflight-errors=Swap
```

```bash

...

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 10.151.30.57:6443 --token 8xomlq.0cdf2pbvjs2gjho3 --discovery-token-ca-cert-hash sha256:92802317cb393682c1d1356c15e8b4ec8af2b8e5143ffd04d8be4eafb5fae368
```

要注意将上面的加入集群的命令保存下面，如果忘记保存上面的 token 和 sha256 值的话也不用担心，我们可以使用下面的命令来查找：

```bash
$ kubeadm token list
```

要查看 CA 证书的 sha256 的值的话，我们可以使用openssl来读取证书获取 sha256 的值：
```bash
$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
e9ca4d9550e698105f1d8fae7ecfd297dd9331ca7d50b5493fa0491b2b4df40c
```

另外还需要注意的是当前版本的 kubeadm 支持的docker版本最大是 17.03，所以要注意下。 上面的信息记录了 kubeadm 初始化整个集群
的过程，生成相关的各种证书、kubeconfig 文件、bootstraptoken 等等，后边是使用kubeadm join往集群中添加节点时用到的命令，
下面的命令是配置如何使用kubectl访问集群的方式：
```bash
mkdir -p /root/.kube/
cp -i /etc/kubernetes/admin.conf /root/.kube/config
chown root:root /root/.kube/config
```
我们根据上面的提示配置好 kubectl 后，就可以使用 kubectl 来查看集群的信息了：
```bash
# 查看集群状态
$ kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health": "true"}

$ kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
node-csr-8qygb8Hjxj-byhbRHawropk81LHNPqZCTePeWoZs3-g   1h        system:bootstrap:8xomlq   Approved,Issued

# 查看集群节点
$ kubectl get nodes
NAME           STATUS    ROLES     AGE       VERSION
ydzs-master1   Ready     master    3h        v1.10.0
```

如果你的集群安装过程中遇到了其他问题，我们可以使用下面的命令来进行重置：
```bash
$ kubeadm reset
$ ifconfig cni0 down && ip link delete cni0
$ ifconfig flannel.1 down && ip link delete flannel.1
$ rm -rf /var/lib/cni/
```

### 9. 安装 Pod Network
接下来我们来安装flannel网络插件，很简单，和安装普通的 POD 没什么两样：
```bash
$ wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
$ kubectl apply -f  kube-flannel.yml
clusterrole.rbac.authorization.k8s.io "flannel" created
clusterrolebinding.rbac.authorization.k8s.io "flannel" created
serviceaccount "flannel" created
configmap "kube-flannel-cfg" created
daemonset.extensions "kube-flannel-ds" created
```

另外需要注意的是如果你的节点有多个网卡的话，需要在 kube-flannel.yml 中使用--iface参数指定集群主机内网网卡的名称，
否则可能会出现 dns 无法解析。flanneld 启动参数加上--iface=<iface-name>
```bash
args:
- --ip-masq
- --kube-subnet-mgr
- --iface=eth0
```

安装完成后使用 kubectl get pods 命令可以查看到我们集群中的组件运行状态，如果都是Running 状态的话，那么恭喜你，
你的 master 节点安装成功了。
```bash
$ kubectl get pods --all-namespaces
NAMESPACE     NAME                                   READY     STATUS    RESTARTS   AGE
kube-system   etcd-ydzs-master1                      1/1       Running   0          10m
kube-system   kube-apiserver-ydzs-master1            1/1       Running   0          10m
kube-system   kube-controller-manager-ydzs-master1   1/1       Running   0          10m
kube-system   kube-dns-86f4d74b45-f5595              3/3       Running   0          10m
kube-system   kube-flannel-ds-qxjs2                  1/1       Running   0          1m
kube-system   kube-proxy-vf5fg                       1/1       Running   0          10m
kube-system   kube-scheduler-ydzs-master1            1/1       Running   0          10m
```

### 10. 添加节点
同样的上面的环境配置、docker 安装、kubeadmin、kubelet、kubectl 这些都在Node(10.151.30.62)节点安装配置好过后，
我们就可以直接在 Node 节点上执行kubeadm join命令了（上面初始化的时候有），同样加上参数--ignore-preflight-errors=Swap: 
```bash
$ kubeadm join 10.151.30.57:6443 --token 8xomlq.0cdf2pbvjs2gjho3 --discovery-token-ca-cert-hash sha256:92802317cb393682c1d1356c15e8b4ec8af2b8e5143ffd04d8be4eafb5fae368 --ignore-preflight-errors=Swap
[preflight] Running pre-flight checks.
    [WARNING Swap]: running with swap on is not supported. Please disable swap
    [WARNING FileExisting-crictl]: crictl not found in system path
Suggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl
[discovery] Trying to connect to API Server "10.151.30.57:6443"
[discovery] Created cluster-info discovery client, requesting info from "https://10.151.30.57:6443"
[discovery] Requesting info from "https://10.151.30.57:6443" again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server "10.151.30.57:6443"
[discovery] Successfully established connection with API Server "10.151.30.57:6443"

This node has joined the cluster:
* Certificate signing request was sent to master and a response
  was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the master to see this node join the cluster.
```

如果出现下面的错误信息：[discovery] Failed to request cluster info, will try again: 
[Get https://10.151.30.27:6443/api/v1/namespaces/kube-public/configmaps/cluster-info: x509: certificate has expired or is not yet valid]，
应该是 master 和 node 之间时间不同步造成的，执行 ntp 时间同步后，重新 init、join 即可。

如果 join 的时候出现下面的错误信息：
[ERROR CRI]: unable to check if the container runtime at "/var/run/dockershim.sock" is running: fork/exec /usr/bin/crictl -r /var/run/dockershim.sock info: no such file or directory，
这个是因为 cri-tools 版本造成的错误，可以卸载掉即可：yum remove cri-tools。
我们可以看到该节点已经加入到集群中去了，然后我们把 master 节点的~/.kube/config文件拷贝到当前节点对应的位置即可使用 kubectl 命令行工具了。


### 遇到的问题
* master节点kubectl get nodes的状态显示NotReady
 1. 查看日志：
    ```bash
    journalctl -f -u kubelet.service
    ```
    日志中有句：
    ```
    NotReady message:docker: network plugin is not ready: cni config uninitialized
    ```
    
    解决：
    参考第9点的配置文件加入- --iface=eth0参数即可
    
* node节点无法join到master节点上

  1. 原因是node节点的hostname和master的hostname相同
  
    解决：
    更改node节点的hostname



