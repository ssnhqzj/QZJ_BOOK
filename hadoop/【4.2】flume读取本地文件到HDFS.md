# flume读取本地文件到HDFS

> 接上篇【4.1】flume安装配置

## 创建flume-hdfs.conf文件

在job目录中创建配置文件flume-hdfs.conf，内容如下：

```shell
# 1.定义agent的名字a2
a2.sources = r2
a2.sinks = k2
a2.channels = c2

#2.定义Source
a2.sources.r2.type = exec
a2.sources.r2.command = tail -F /opt/test.log
a2.sources.r2.shell = /bin/bash -c

#3.定义sink
a2.sinks.k2.type = hdfs
a2.sinks.k2.hdfs.path = hdfs://bigdata111:9000/flume/%H

#写入hdfs的文件名的前缀
a2.sinks.k2.hdfs.filePrefix = test-log-
#是否按照时间滚动文件夹
a2.sinks.k2.hdfs.round = true
#多少时间单位创建一个新的文件夹
a2.sinks.k2.hdfs.roundValue = 1
#重新定义时间单位
a2.sinks.k2.hdfs.roundUnit = hour
#是否在hfds上目录使用本地时间戳
a2.sinks.k2.hdfs.useLocalTimeStamp = true
#积攒多少个Event才flush到HDFS一次,不能大于a2.channels.c2.transactionCapacity配置
a2.sinks.k2.hdfs.batchSize = 100
#设置文件类型，当使用DataStream时候，文件不会被压缩。使用文本文件，不使用sequenceFile
a2.sinks.k2.hdfs.fileType = DataStream
#多长时间写数据到新文件；600秒。hdfs间隔多长将临时文件滚动成最终目标文件，单位：秒；滚动（roll）指的是，hdfs sink将临时文件重命名成最终目标文件，并新打开一个临时文件来写入数据
a2.sinks.k2.hdfs.rollInterval = 600
#设置每个文件的滚动大小。文件达到多少数据量时，写新文件。当临时文件达到该大小（单位：bytes）时，滚动成目标文件。
a2.sinks.k2.hdfs.rollSize = 134217700
#当events数据达到该数量时候，将临时文件滚动成目标文件；如果设置成0，则表示不根据events数量来滚动文件；默认值：10
a2.sinks.k2.hdfs.rollCount = 0
#最小副本数
a2.sinks.k2.hdfs.minBlockReplicas = 1

# 4.定义Channel
a2.channels.c2.type = memory
a2.channels.c2.capacity = 1000
a2.channels.c2.transactionCapacity = 100

# 5.链接
a2.sources.r2.channels = c2
a2.sinks.k2.channel = c2
```

> 注意：
>
> a2.sources.r2.type = exec#低于1.6的版本都是这个，表示文件类型
> a2.sources.r2.command = tail -F /opt/Andy#tail -F 监控 文件名
> a2.sources.r2.shell = /bin/bash -c#写死的，不用更改
> a2.sinks.k2.hdfs.path = hdfs://bigdata111:9000/flume/%H#%H 当前的小时数
> a2.sinks.k2.hdfs.rollSize = 134217700 #128MB。当临时文件达到该大小（单位：bytes）时，滚动成目标文件。
> 这里指的是每隔一小时会产生一个新的文件夹。比如现在是/flume/05,一小时后会产生/flume/06
> a2.sinks.k2.hdfs.roundValue = 1
> a2.sinks.k2.hdfs.roundUnit = hour

## 执行监控配置

```shell
bin/flume-ng agent \
--conf conf/ \
--name a2 \
--conf-file job/flume-hdfs.conf
```

> 1. NoClassDefFoundError报错
>
>    ```shell
>    Failed to start agent because dependencies were not found in classpath. Error follows.
>    java.lang.NoClassDefFoundError: org/apache/hadoop/io/SequenceFile$CompressionType
>    # 原因SequenceFile$CompressionType对应缺少{HADOOP_HOME}share/hadoop/common/hadoop-common-2.4.0.jar但是还有很多同样的问题。
>    # 解决:
>    cp {HADOOP_HOME}share/hadoop/common/*.jar ${FLUME_HOME}/lib/
>    cp {HADOOP_HOME}share/hadoop/common/lib/*.jar ${FLUME_HOME}/lib/
>    ```
>
>    
>
> 2. 错No FileSystem for scheme: hdfs
>
>    ```shell
>    # 原因可能是缺少hdfs jar
>    # 解决:
>    cp {HADOOP_HOME}share/hadoop/hdfs/hadoop-hdfs-2.4.1.jar ${FLUME_HOME}/lib/
>    # 也有可能是版本匹配问题
>    ```
>
>    