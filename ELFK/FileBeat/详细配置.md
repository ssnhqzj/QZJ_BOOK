```
filebeat.prospectors:
- input_type: log
  enabled: True
  paths:
    - /var/log/mysql-slow-*
#这个地方是关键，我们给上边日志加上了tags，方便在logstash里边通过这个tags 过滤并格式化自己想要的内容；
  tags: ["mysql_slow_logs"]
#有的时候日志不是一行输出的，如果不用multiline的话，会导致一条日志被分割成多条收集过来，形成不完整日志，这样的日志对于我们来说是没有用的！通过正则匹配语句开头，这样multiline 会在匹配开头
之后，一直到下一个这样开通的语句合并成一条语句。
#pattern：多行日志开始的那一行匹配的pattern
#negate：是否需要对pattern条件转置使用，不翻转设为true，反转设置为false
#match：匹配pattern后，与前面（before）还是后面（after）的内容合并为一条日志
#max_lines：合并的最多行数（包含匹配pattern的那一行 默认值是500行
#timeout：到了timeout之后，即使没有匹配一个新的pattern（发生一个新的事件），也把已经匹配的日志事件发送出去
  multiline.pattern: '^\d{4}/\d{2}/\d{2}'  (2018\05\01  我的匹配是已这样的日期开头的）
  multiline.negate: true
  multiline.match: after
  multiline.Max_lines:20
  multiline.timeout: 10s
- input_type: log
  paths:
    - /var/log/mysql-sql-*
  tags: ["mysql_sql_logs"]
  multiline.pattern: '^\d{4}/\d{2}/\d{2}'
  multiline.negate: true
  multiline.match: after
  multiline.timeout: 10s
  encoding: utf-8
  document_type: mysql-proxy
  scan_frequency: 20s
  harverster_buffer_size: 16384
  max_bytes: 10485760
  tail_files: true
 #tail_files：如果设置为true，Filebeat从文件尾开始监控文件新增内容，把新增的每一行文件作为一个事件依次发送，而不是从文件开始处重新发送所有内容。默认是false；
 ```