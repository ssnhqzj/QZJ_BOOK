```
input {
  kafka {
    topics_pattern => "mysql.*"
    bootstrap_servers => "x.x.x.x:9092"
    #auto_offset_rest => "latest"
    codec => json
    group_id => "logstash-g1"
   }
}

#终于到了关键的地方了，logstash的filter ，使用filter 过滤出来我们想要的日志，
filter {
     #if 还可以使用or 或者and 作为条件语句，举个栗子： if “a” or “b”  or “c” in [tags]，
     #这样就可以过滤多个tags 的标签了，我们这个主要用在同型号的交换设备的日志正规化，
     #比如说你有5台交换机，把日志指定到了同一个syslog-ng 上，收集日志的时候只能通过同一个filebeat，
     #多个prospector加不同的tags。这个时候过滤就可以通过判断相应的tags来完成了。
    if "mysql_slow_logs" in [tags]{ 
    grok {
     #grok 里边有定义好的现场的模板你可以用，但是更多的是自定义模板，规则是这样的，
     #小括号里边包含所有一个key和value，例子：（?<key>value），比如以下的信息，第一个我定义的key是data，
     #表示方法为：?<key> 前边一个问号，然后用<>把key包含在里边去。value就是纯正则了，这个我就不举例子了。
     #这个有个在线的调试库，可以供大家参考，http://grokdebug.herokuapp.com/ 
        match => { "message" => "(?<date>\d{4}/\d{2}/\d{2}\s(?<datetime>%{TIME}))\s-\s(?<status>\w{2})\s-\s(?<respond_time>\d+)\.\d+\w{2}\s-\s%{IP:client}:(?<client-port>\d+)\[\d+\]->%{IP:server}:(?<server-port>\d+).*:(?<databases><\w+>):(?<SQL>.*)"}
        #过滤完成之后把之前的message 移出掉，节约磁盘空间。
        remove_field => ["message"]
    }
  }
    else if "mysql_sql_logs" in [tags]{
    grok {
        match => { "message" => "(?<date>\d{4}/\d{2}/\d{2}\s(?<datetime>%{TIME}))\s-\s(?<status>\w{2})\s-\s(?<respond_time>\d+\.\d+)\w{2}\s-\s%{IP:client}:(?<client-port>\d+)\[\d+\]->%{IP:server}:(?<server-port>\d+).*:(?<databases><\w+>):(?<SQL>.*)"}
        remove_field => ["message"]}
  }
}
```